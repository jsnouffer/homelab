llama:
  enabled: true
  global:
    fullnameOverride: &name llama
  defaultPodOptions:
    automountServiceAccountToken: false
    dnsConfig:
      options:
        - name: ndots
          value: "3"
  controllers:
    backend:
      type: deployment
      replicas: 0
      pod:
        runtimeClassName: nvidia
        tolerations:
          - key: nvidia.com/gpu
            effect: NoSchedule
            operator: Exists
      containers:
        main:
          image:
            repository: ghcr.io/ggerganov/llama.cpp
            tag: server-cuda--b1-917dc8c
            pullPolicy: IfNotPresent
          args:
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8080"
            - "-m"
            - "/app/models/Phi-3-mini-4k-instruct-fp16.gguf"
            - "-ngl"
            - "99"
            - "--timeout"
            - "900"
            - "--log-disable"
            - "--mlock"
          env:
            TZ: "America/New_York"
            LLAMA_CUDA_KQUANTS_ITER: 1
          resources:
            limits:
              cpu: "6"
              memory: 12Gi
              nvidia.com/gpu: 1
            requests:
              cpu: "1"
              memory: 2Gi
              nvidia.com/gpu: 1
          probes: &probes
            liveness: &lr-probe
              enabled: true
              type: &probe-type HTTP
              path: &probe-path /health
              spec:
                initialDelaySeconds: 0
                periodSeconds: 10
                timeoutSeconds: 1
                failureThreshold: 3
            readiness: *lr-probe
            startup:
              enabled: true
              type: *probe-type
              path: *probe-path
              spec:
                initialDelaySeconds: 20
                periodSeconds: 10
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 30
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
                - SYS_RESOURCE
    webui:
      type: deployment
      replicas: 0
      containers:
        main:
          image:
            repository: ghcr.io/open-webui/open-webui
            tag: "main"
            pullPolicy: IfNotPresent
          env:
            TZ: "America/New_York"
            OPENAI_API_BASE_URL: http://llama-backend:8080/v1
            OPENAI_API_KEYS: "none"
            WEBUI_AUTH: "false"
            WEBUI_URL: https://openwebui.jsnouff.net
            ENABLE_LITELLM: "false"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 150m
              memory: 256Mi
          probes: *probes
  service:
    backend:
      controller: backend
      annotations:
        traefik.ingress.kubernetes.io/service.serversscheme: http
      ports:
        http:
          port: &backend-port 8080
    webui:
      controller: webui
      annotations:
        traefik.ingress.kubernetes.io/service.serversscheme: http
      ports:
        http:
          port: &webui-port 8080
  ingress:
    backend:
      enabled: true
      hosts:
        - host: &backend-domain llama-cpp.jsnouff.net
          paths:
            - path: /
              pathType: Prefix
              service:
                name: llama-backend
                port: *backend-port
      tls:
        - hosts:
            - *backend-domain
          secretName: backend-tls-secret
      annotations: &ingress-annotations
        cert-manager.io/cluster-issuer: lets-encrypt-production
        ingress.kubernetes.io/force-ssl-redirect: "true"
        ingress.kubernetes.io/protocol: http
        kubernetes.io/ingress.class: traefik
        kubernetes.io/tls-acme: "true"
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        traefik.ingress.kubernetes.io/router.middlewares: traefik-default-security@kubernetescrd
        traefik.ingress.kubernetes.io/router.tls.options: traefik-default-tls@kubernetescrd
        traefik.ingress.kubernetes.io/router.tls: "true"
    webui:
      enabled: true
      hosts:
        - host: &webui-domain openwebui.jsnouff.net
          paths:
            - path: /
              pathType: Prefix
              service:
                name: llama-webui
                port: *webui-port
      tls:
        - hosts:
            - *webui-domain
          secretName: webui-tls-secret
      annotations: *ingress-annotations
  persistence:
    models:
      enabled: true
      existingClaim: llm-nfs
      advancedMounts:
        backend:
          main:
            - path: /app/models
              subPath: llm