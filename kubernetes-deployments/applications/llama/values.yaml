llama:
  enabled: true
  global:
    fullnameOverride: &name llama
  defaultPodOptions:
    automountServiceAccountToken: false
    dnsConfig:
      options:
        - name: ndots
          value: "3"
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    tolerations:
      - key: nvidia.com/gpu
        effect: NoSchedule
        operator: Exists
    runtimeClassName: nvidia
  controllers:
    llama:
      type: deployment
      initContainers:
        model-init:
          image:
            repository: "quay.io/prometheus/busybox"
            tag: "latest"
            pullPolicy: IfNotPresent
          command: ["sh", "-c", "chown 1234:1234 /app/models -Rf && ls -lsatr /app/models/"]
      containers:
        main:
          image:
            repository: ghcr.io/ggerganov/llama.cpp
            tag: server-cuda--b1-917dc8c
            pullPolicy: IfNotPresent
          args: ["--host", "0.0.0.0","-hfr","microsoft/Phi-3-mini-4k-instruct-gguf","-hff","Phi-3-mini-4k-instruct-fp16.gguf","-n","512", "-ngl","99", "-c", "8192", "-t", "8","--timeout", "900", "--log-disable", "--mlock"]
          env:
            TZ: "America/New_York"
            LLAMA_CUDA_KQUANTS_ITER: 1
          resources:
            limits:
              cpu: "8"
              memory: 12Gi
              nvidia.com/gpu: 1
            requests:
              cpu: "1"
              memory: 2Gi
              nvidia.com/gpu: 1
          probes:
            liveness: &lr-probe
              enabled: true
              type: &probe-type HTTP
              path: &probe-path /health
              spec:
                initialDelaySeconds: 0
                periodSeconds: 10
                timeoutSeconds: 1
                failureThreshold: 3
            readiness: *lr-probe
            startup:
              enabled: true
              type: *probe-type
              path: *probe-path
              spec:
                initialDelaySeconds: 20
                periodSeconds: 10
                timeoutSeconds: 5
                successThreshold: 1
                failureThreshold: 30
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
  service:
    main:
      controller: llama
      annotations:
        traefik.ingress.kubernetes.io/service.serversscheme: http
      ports:
        http:
          port: &main-port 8080
  ingress:
    main:
      enabled: true
      hosts:
        - host: &domain llama.jsnouff.net
          paths:
            - path: /
              pathType: Prefix
              service:
                name: *name
                port: *main-port
      tls:
        - hosts:
            - *domain
          secretName: tls-secret
      annotations:
        cert-manager.io/cluster-issuer: lets-encrypt-production
        ingress.kubernetes.io/force-ssl-redirect: "true"
        ingress.kubernetes.io/protocol: http
        kubernetes.io/ingress.class: traefik
        kubernetes.io/tls-acme: "true"
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        traefik.ingress.kubernetes.io/router.middlewares: traefik-default-security@kubernetescrd
        traefik.ingress.kubernetes.io/router.tls.options: traefik-default-tls@kubernetescrd
        traefik.ingress.kubernetes.io/router.tls: "true"
  persistence:
    data:
      enabled: true
      accessMode: ReadWriteOnce
      size: "30Gi"
      advancedMounts:
        llama:
          main:
            - path: /app/models
          model-init:
            - path: /app/models